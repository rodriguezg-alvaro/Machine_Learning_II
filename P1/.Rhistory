monitor = ESUMonitor   #  <----------------------  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
)
## Load libraries ---------------------------------------------------------------
library(caret)
library(ggplot2)
library(GA)  # install.packages("GA")
## remove all epxisting variables--------------------------------------------------
rm(list=ls())
fcreate <- function(coef1, coef2){function(x) coef1*x+coef2}
f <- fcreate(1,5)
minVal = -20; maxVal = 20;
curve(f, minVal, maxVal)
x <- seq(minVal, maxVal, by = 0.1)
fval <- f(x)
noise <- runif(length(x),-3,3)
fnoise<-noise
freal<-fcreate(1,5)
yreal<- freal(x)+fnoise
data=data.frame(x,yreal)
# show the dataset and the theoretical line
p<- ggplot() + geom_point(aes(x=x, y=yreal)) + labs(title = "Problem: y = b1 * x + b0 ") + geom_abline(aes(intercept=5, slope=1, color="theoretical" ))
plot(p)
y_est=lm(yreal~x,data=data)
summary(y_est)
coef=y_est$coefficients
y_estim=coef[1]+coef[2]*x
rmse_lm=mean((yreal-y_estim)^2)
rmse_lm
p<- ggplot() + geom_point(aes(x=x, y=yreal)) + labs(title = "Problem: y = b1 * x + b0 ") +
geom_abline(aes(intercept=5, slope=1, color="theoretical" ))+
geom_smooth(data=data,aes(x,yreal,color='regresiÃ³n lineal'),se=FALSE)
plot(p)
myfitness <- function(coef) {
festimated<-fcreate(coef[1], coef[2])
yestimated = festimated(x)
-mean((yreal-yestimated)^2) # Negative (GA maximizes fitness)
}
myfitness(c(1,1))
myfitness(c(0,1))
myfitness(c(1,5))
ESUMonitor <- function(obj)
{
lines <- as.data.frame(obj@population)
lines$ind = rownames(lines)
p<- ggplot() + geom_point(aes(x=x, y=yreal)) + labs(title = paste("Iteration =", obj@iter)) + geom_abline(aes(intercept=lines[,2], slope=lines[,1], color="red" ))
print(p)
Sys.sleep(0.1)
}
# Run Basic GA using the monitor (see the Plots)
GA <- ga(type = "real-valued", fitness = myfitness,
lower = c(-3, -3), upper = c(3,0),
popSize = 100,   #  <-------  Number of individuals
pcrossover = 0.8,
pmutation = 0.1,
maxiter = 100,         #  <----------------------  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
monitor = ESUMonitor   #  <----------------------  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
)
# Run Basic GA using the monitor (see the Plots)
GA <- ga(type = "real-valued", fitness = myfitness,
lower = c(-3, -3), upper = c(0,8),
popSize = 100,   #  <-------  Number of individuals
pcrossover = 0.8,
pmutation = 0.1,
maxiter = 100,         #  <----------------------  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
monitor = ESUMonitor   #  <----------------------  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
)
# Run Basic GA using the monitor (see the Plots)
GA <- ga(type = "real-valued", fitness = myfitness,
lower = c(-3, -3), upper = c(-8,8),
popSize = 100,   #  <-------  Number of individuals
pcrossover = 0.8,
pmutation = 0.1,
maxiter = 100,         #  <----------------------  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
monitor = ESUMonitor,  #  <----------------------  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
optim = TRUE
)
# Run Basic GA using the monitor (see the Plots)
GA <- ga(type = "real-valued", fitness = myfitness,
lower = c(-3, -3), upper = c(8,8),
popSize = 100,   #  <-------  Number of individuals
pcrossover = 0.8,
pmutation = 0.1,
maxiter = 100,         #  <----------------------  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
monitor = ESUMonitor,   #  <----------------------  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
optim = TRUE
)
abline(v = GA@solution, lty = 3, col = 2)
plot(GA)
summary(GA)
plot(GA)
summary(GA)
abline(v = GA@solution, lty = 3, col = 2)
plot(GA)
summary(GA)
print(GA@solution)
lines <- as.data.frame(GA@population)
lines$ind = rownames(lines)
p<-ggplot() + geom_point(aes(x=x, y=yreal))+ geom_abline(aes(intercept=lines[,2], slope=lines[,1], color="red" ))
print(p)
lines <- as.data.frame(GA@population)
lines$ind = rownames(lines)
p<-ggplot() + geom_point(aes(x=x, y=yreal))+ geom_abline(aes(intercept=lines[,2], slope=lines[,1], color="red" ))
print(p)
print(GA@solution)
plot(GA)
#################################################################################
############## TASK2-ASSESSMENT 1 ENSEMBLE METHODS   ############################
##############        MACHINE LEARNING II            ############################
##############           ESU   Feb-2022              ############################
#################################################################################
install.packages("MLTools_0.0.31.tar.gz", repos = NULL, dep = TRUE)
## Load libraries ---------------------------------------------------------------
library(caret)
library(caret)
library(ggplot2)
library(GGally)
library(MLTools)
library(randomForest)
library(gbm)
## remove all epxisting variables--------------------------------------------------
rm(list=ls())
fcirTR = read.table("./Assignment_1/Data/SimDataCIRCLETR.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
fcirTR$Y = as.factor(fcirTR$Y)
fcirTS = read.table("./Assignment_1/Data/SimDataCIRCLETS.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
fcirTS$Y = as.factor(fcirTS$Y)
set.seed(1999)
ratioTR = 1
trainIndex <- createDataPartition(fcirTR$Y,      #output variable. createDataPartition creates proportional partitions
p = ratioTR,  #split probability
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
# Sets de TR
fTR = fcirTR[trainIndex,]
x1_x2 = sqrt(fcirTR$X1^2 + fcirTR$X2^2)
fcirTR$x1_x2 = x1_x2
ggpairs(fcirTR, aes(colour = Y, alpha = 0.4))
set.seed(1999)
ratioTR = 1
trainIndex <- createDataPartition(fcirTR$Y,      #output variable. createDataPartition creates proportional partitions
p = ratioTR,  #split probability
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
# Sets de TR
fTR = fcirTR[trainIndex,]
ctrl <- trainControl(method = "none",                      # method= "none" when no resampling is used
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples
classProbs = TRUE,                    #Compute class probs in Hold-out samples
returnResamp = "all",                 #Return all information about resampling
savePredictions = TRUE)               #Compute class probs in Hold-out samples
# with cross-validation
ctrl_tune <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples
classProbs = TRUE,                    #Compute class probs in Hold-out samples
returnResamp = "all",                 #Return all information about resampling
savePredictions = TRUE)
ORIGINALtree_2.fit = train(fTR[,c("X1","X2","X3","X4","X5","X6", "x1_x2")],
y = fTR$Y,
method = "rpart",
parms = list(split = "gini"),
#tuneGrid = data.frame(cp = seq(0,0.4,0.01)),
tuneGrid = data.frame(cp = seq(0.0,0.01,0.001)),
trControl = ctrl_tune,
metric = "Accuracy")
ORIGINALtree_2.fit #information about the resampling settings
summary(ORIGINALtree_2.fit)  #information about the model trained
ORIGINALtree_2.fit$finalModel #Cuts performed and nodes. Also shows the number and percentage of cases in each node.
#Plot the tree:
plot(ORIGINALtree_2.fit$finalModel, uniform=TRUE,margin=0.2)
text(ORIGINALtree_2.fit$finalModel, use.n=TRUE, all=TRUE, cex=0.8)
#Measure for variable importance
varImp(ORIGINALtree_2.fit,scale = FALSE)
plot(varImp(ORIGINALtree_2.fit,scale = FALSE))
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and validation sets
#training
fTR_eval = fTR
fTR_eval$tree_prob = predict(ORIGINALtree_2.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$tree_pred = predict(ORIGINALtree_2.fit, type="raw" , newdata = fTR) # predict classes
#test
fTS <- fcirTS
fTS_eval = fTS
fTS_eval$tree_prob = predict(ORIGINALtree_2.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$tree_pred = predict(ORIGINALtree_2.fit, type="raw" , newdata = fTS) # predict classes
fTS_eval$tree_prob = predict(ORIGINALtree_2.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval = fTS
fTS_eval$tree_prob = predict(ORIGINALtree_2.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$tree_pred = predict(ORIGINALtree_2.fit, type="raw" , newdata = fTS) # predict classes
## Performance measures: confusion matices
# Training
confusionMatrix(data = fTR_eval$tree_pred, reference = fTR_eval$Y, positive = "I")
# Test
confusionMatrix(fTS_eval$tree_pred,  fTS_eval$Y, positive = "I")
fcirTS$Y = as.factor(fcirTS$Y)
# Test
confusionMatrix(fTS_eval$tree_pred,  fTS_eval$Y, positive = "I")
#test
fTS <- fcirTS
fTS_eval = fTS
# Test
confusionMatrix(fTS_eval$tree_pred,  fTS_eval$Y, positive = "I")
#################################################################################
############## TASK2-ASSESSMENT 1 ENSEMBLE METHODS   ############################
##############        MACHINE LEARNING II            ############################
##############           ESU   Feb-2022              ############################
#################################################################################
install.packages("MLTools_0.0.31.tar.gz", repos = NULL, dep = TRUE)
## Set working directory --------------------------------------------------------
getwd()
## Load libraries ---------------------------------------------------------------
library(caret)
library(ggplot2)
library(GGally)
library(MLTools)
library(randomForest)
library(gbm)
## remove all epxisting variables--------------------------------------------------
rm(list=ls())
#################################################################################
# LOAD DATASETS
#################################################################################
#
fcirTR = read.table("./Assignment_1/Data/SimDataCIRCLETR.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
fcirTR$Y = as.factor(fcirTR$Y)
## Exploratory analysis -------------------------------------------------------------------------------------
ggpairs(fcirTR, aes(colour = Y, alpha = 0.4))
ggplot(fcirTR)+geom_point(aes(x=X1,y=X2,color=Y))
fcirTS = read.table("./Assignment_1/Data/SimDataCIRCLETS.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
fcirTS$Y = as.factor(fcirTS$Y)
## Exploratory analysis -------------------------------------------------------------------------------------
ggpairs(fcirTR, aes(colour = Y, alpha = 0.4))
ggplot(fcirTS)+geom_point(aes(x=X1,y=X2,color=Y))
#################################################################################
# SET TRAINCONTROLS for Caret
#################################################################################
set.seed(1999)
ratioTR = 1
trainIndex <- createDataPartition(fcirTR$Y,      #output variable. createDataPartition creates proportional partitions
p = ratioTR,  #split probability
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
# Sets de TR
fTR = fcirTR[trainIndex,]
# with cross-validation
ctrl_tune <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples
classProbs = TRUE,                    #Compute class probs in Hold-out samples
returnResamp = "all",                 #Return all information about resampling
savePredictions = TRUE)               #Compute class probs in Hold-out samples
#################################################################################
# FIT BASIC CLASSIFICATION TREE (THE NAIVE APPROACH?)
#################################################################################
tree.fit = train(fTR[,c("X1","X2","X3","X4","X5","X6")], #Input variables. Other option: fdata[,1:2]
y = fTR$Y,
method = "rpart",
parms = list(split = "gini"),          # impuriry measure
#tuneGrid = data.frame(cp = seq(0,0.4,0.01)), # first trial
tuneGrid = data.frame(cp = seq(0.0,0.01,0.001)), # detail
trControl = ctrl_tune,    #ctrl_tune
metric = "Accuracy")
tree.fit #information about the resampling settings
summary(tree.fit)  #information about the model trained
tree.fit$finalModel #Cuts performed and nodes. Also shows the number and percentage of cases in each node.
#Plot the tree:
plot(tree.fit$finalModel, uniform=TRUE,margin=0.2)
text(tree.fit$finalModel, use.n=TRUE, all=TRUE, cex=0.8)
#Measure for variable importance
varImp(tree.fit,scale = FALSE)
g1 <- plot(varImp(tree.fit,scale = FALSE))
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and validation sets
#training
fTR_eval = fTR
fTR_eval$tree_prob = predict(tree.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$tree_pred = predict(tree.fit, type="raw" , newdata = fTR) # predict classes
#test
fTS <- fcirTS
fTS_eval = fTS
fTS_eval$tree_prob = predict(tree.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$tree_pred = predict(tree.fit, type="raw" , newdata = fTS) # predict classes
## Performance measures: confusion matices
# Training
confusionMatrix(data = fTR_eval$tree_pred, reference = fTR_eval$Y, positive = "I")
# Test
confusionMatrix(fTS_eval$tree_pred,  fTS_eval$Y, positive = "I")
#################################################################################
# FIT BAGGED TREE
#################################################################################
set.seed(150) #For replication
#Train model using boostrapped trained data
BAGtree.fit = train(fTR[,c("X1","X2","X3", "X4", "X5", "X6")], #Input variables. Other option: fdata[,1:2]
y = fTR$Y,
method = "treebag",
trControl = ctrl_tune,
metric = "Accuracy")
BAGtree.fit
summary(BAGtree.fit)
#Measure for variable importance
varImp(BAGtree.fit,scale = FALSE)
g2 <- plot(varImp(BAGtree.fit,scale = FALSE))
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and validation sets
#training
#fTR_eval = fTR
fTR_eval$BAGtree_prob = predict(BAGtree.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$BAGtree_pred = predict(BAGtree.fit, type="raw" , newdata = fTR) # predict classes
#validation
#fTV_eval = fTV
fTS_eval$BAGtree_prob = predict(BAGtree.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$BAGtree_pred = predict(BAGtree.fit, type="raw" , newdata = fTS) # predict classes
#######confusion matices
# Training
confusionMatrix(data = fTR_eval$BAGtree_pred, reference = fTR_eval$Y, positive = "I")
# Test
confusionMatrix(fTS_eval$BAGtree_pred, fTS_eval$Y, positive = "I")
#
# #######Classification performance plots
# # Training
PlotClassPerformance(fTR_eval$Y,       #Real observations
fTR_eval$BAGtree_prob,  #predicted probabilities
selClass = "I") #Class to be analyzed
# # Validation
PlotClassPerformance(fTS_eval$Y,       #Real observations
fTS_eval$BAGtree_prob,  #predicted probabilities
selClass = "I") #Class to be analyzed)
#################################################################################
# FIT RANDOM FORESTS
#################################################################################
set.seed(150) #For replication
#Training the model
RFtree.fit = train(fTR[,c("X1","X2","X3","X4", "X5", "X6")],
y = fTR$Y, # output variable
method = "rf",
ntree = 100,
tuneGrid = data.frame(mtry = seq(1,ncol(fTR)-1)), # m parameter
trControl = ctrl_tune, #Resampling settings
metric = "Accuracy")    #Summary metrics
# See the forest
RFtree.fit
varImp(RFtree.fit,scale = FALSE)
g3 <- plot(varImp(RFtree.fit,scale = FALSE))
#Evaluate the model with training and validation sets
#training
#fTR_eval = fTR
fTR_eval$RFtree_prob = predict(RFtree.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$RFtree_pred = predict(RFtree.fit, type="raw" , newdata = fTR) # predict classes
#validation
#fTV_eval = fTV
fTS_eval$RFtree_prob = predict(RFtree.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$RFtree_pred = predict(RFtree.fit, type="raw" , newdata = fTS) # predict classes
# Training
confusionMatrix(data = fTR_eval$RFtree_pred, reference = fTR_eval$Y, positive = "I")
# Test
confusionMatrix(fTS_eval$RFtree_pred, fTS_eval$Y,  positive = "I")
# #######Classification performance plots
# # Training
PlotClassPerformance(fTR_eval$Y, fTR_eval$RFtree_prob,  selClass = "I")
# # Test
PlotClassPerformance(fTS_eval$Y, fTS_eval$RFtree_prob,  selClass = "I")
#################################################################################
# FIT GRADIENT BOOSTING
#################################################################################
seedTRTVpartition = 150
gbmGrid <- expand.grid(.n.trees = seq(100, 1000, by = 50),
.interaction.depth = seq(1, 10, by = 1),
.shrinkage = c(0.01, 0.1), # two typical values
.n.minobsinnode = c(1))
# Train the GBM
set.seed(150)
GBM.fit =  train(fTR[,c("X1","X2","X3","X4","X5","X6")], #Input variables. Other option: fdata[,1:2]
y = fTR$Y,
method = "gbm",
tuneGrid = gbmGrid,
trControl = ctrl_tune,    #use cv
metric = "Accuracy",
verbose = FALSE) # The gbm() function produces copious amounts of output, avoid printing a lot
GBM.fit #information about the resampling settings
plot(GBM.fit)
#Measure for variable importance
varImp(GBM.fit,scale = FALSE)
plot(varImp(GBM.fit,scale = FALSE))
#Plot OOB
plot(GBM.fit$finalModel$oobag.improve) # shows the oobag improvement evolving with n.trees
GBM.fit;
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and validation sets
#training
fTR_eval$GBtree_prob = predict(GBM.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$GBtree_pred = predict(GBM.fit, type="raw" , newdata = fTR) # predict classes
#validation
fTS_eval$GBtree_prob = predict(GBM.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$GBtree_pred = predict(GBM.fit, type="raw" , newdata = fTS) # predict classes
## Performance measures: confusion matices
confusionMatrix(fTR_eval$GBtree_pred, fTR_eval$Y, positive = "I") # Training
confusionMatrix(fTS_eval$GBtree_pred,  fTS_eval$Y, positive = "I")# Validation
#################################################################################
# COMPARE USING TS
#################################################################################
library(caret)
library(ggplot2)
library(ROCR)
library(MLTools)
library(GGally)
library(dplyr)
library(tidyverse)
#TEST
#Overall accuracy
confusionMatrix(fTS_eval$tree_pred, fTS_eval$Y, positive = "I")$overall[1]
confusionMatrix(fTS_eval$BAGtree_pred, fTS_eval$Y, positive = "I")$overall[1]
confusionMatrix(fTS_eval$RFtree_pred, fTS_eval$Y, positive = "I")$overall[1]
confusionMatrix(fTS_eval$GBtree_pred, fTS_eval$Y, positive = "I")$overall[1]
#Calibration curve
calPlotData <- calibration(Y ~ tree_prob$I + BAGtree_prob$I + RFtree_prob$I + GBtree_prob$I, class = "I",cuts = 6, data = fTS_eval)
xyplot(calPlotData, auto.key = list(columns = 2))
#ROC curve
library(pROC)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$tree_prob$I)
plot(reducedRoc, col="blue")
auc(reducedRoc)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$BAGtree_prob$I)
plot(reducedRoc, add=TRUE, col="red")
auc(reducedRoc)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$RFtree_prob$I)
plot(reducedRoc, add=TRUE, col="orange")
auc(reducedRoc)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$GBtree_prob$I)
plot(reducedRoc, add=TRUE, col="green")
auc(reducedRoc)
legend("bottomright", legend=c("Original tree", "Bagged Tree", "Random Forest","Gradient Boosting"), col=c("blue", "red", "orange","green"), lwd=2)
g1
g2
g3
#################################################################################
# FIT  CLASSIFICATION TREE (THE NAIVE APPROACH?)
#################################################################################
x1_x2 = sqrt(fcirTR$X1^2 + fcirTR$X2^2)
fcirTR$x1_x2 = x1_x2
ggpairs(fcirTR, aes(colour = Y, alpha = 0.4))
set.seed(1999)
ratioTR = 1
trainIndex <- createDataPartition(fcirTR$Y,      #output variable. createDataPartition creates proportional partitions
p = ratioTR,  #split probability
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
# Sets de TR
fTR = fcirTR[trainIndex,]
ctrl <- trainControl(method = "none",                      # method= "none" when no resampling is used
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples
classProbs = TRUE,                    #Compute class probs in Hold-out samples
returnResamp = "all",                 #Return all information about resampling
savePredictions = TRUE)               #Compute class probs in Hold-out samples
# with cross-validation
ctrl_tune <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples
classProbs = TRUE,                    #Compute class probs in Hold-out samples
returnResamp = "all",                 #Return all information about resampling
savePredictions = TRUE)
ORIGINALtree_2.fit = train(fTR[,c("X1","X2","X3","X4","X5","X6", "x1_x2")],
y = fTR$Y,
method = "rpart",
parms = list(split = "gini"),
#tuneGrid = data.frame(cp = seq(0,0.4,0.01)),
tuneGrid = data.frame(cp = seq(0.0,0.01,0.001)),
trControl = ctrl_tune,
metric = "Accuracy")
ORIGINALtree_2.fit #information about the resampling settings
summary(ORIGINALtree_2.fit)  #information about the model trained
ORIGINALtree_2.fit$finalModel #Cuts performed and nodes. Also shows the number and percentage of cases in each node.
#Plot the tree:
plot(ORIGINALtree_2.fit$finalModel, uniform=TRUE,margin=0.2)
text(ORIGINALtree_2.fit$finalModel, use.n=TRUE, all=TRUE, cex=0.8)
#Measure for variable importance
varImp(ORIGINALtree_2.fit,scale = FALSE)
plot(varImp(ORIGINALtree_2.fit,scale = FALSE))
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and validation sets
#training
fTR_eval = fTR
fTR_eval$tree_prob = predict(ORIGINALtree_2.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$tree_pred = predict(ORIGINALtree_2.fit, type="raw" , newdata = fTR) # predict classes
#test
fTS <- fcirTS
fTS_eval = fTS
fTS_eval$tree_prob = predict(ORIGINALtree_2.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$tree_pred = predict(ORIGINALtree_2.fit, type="raw" , newdata = fTS) # predict classes
## Performance measures: confusion matices
# Training
confusionMatrix(data = fTR_eval$tree_pred, reference = fTR_eval$Y, positive = "I")
# Test
confusionMatrix(fTS_eval$tree_pred,  fTS_eval$Y, positive = "I")
## Evaluate model --------------------------------------------------------------------------------
tree_2.fit = ORIGINALtree_2.fit;
#Evaluate the model with training and validation sets
#training
fTR_eval = fTR
fTR_eval$tree_prob = predict(tree_2.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$tree_pred = predict(tree_2.fit, type="raw" , newdata = fTR) # predict classes
#test
x1_x2 = sqrt(fcirTS$X1^2 + fcirTS$X2^2)
fcirTS$x1_x2 = x1_x2
fTS <- fcirTS
fTS_eval = fTS
fTS_eval$tree_prob = predict(tree_2.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$tree_pred = predict(tree_2.fit, type="raw" , newdata = fTS) # predict classes
## Performance measures: confusion matices
# Training
confusionMatrix(data = fTR_eval$tree_pred, reference = fTR_eval$Y, positive = "I")
# Test
confusionMatrix(fTS_eval$tree_pred,  fTS_eval$Y, positive = "I")
ORIGINALtree_2.fit #information about the resampling settings
