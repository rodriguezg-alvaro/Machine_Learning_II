#################################################################################
############## TASK2-ASSESSMENT 1 ENSEMBLE METHODS   ############################
##############        MACHINE LEARNING II            ############################
##############           ESU   Feb-2022              ############################
#################################################################################

## Set working directory --------------------------------------------------------
getwd()

## Load libraries ---------------------------------------------------------------
library(caret)
library(ggplot2)
library(GGally)
library(MLTools)
library(randomForest)
library(gbm)

## remove all epxisting variables--------------------------------------------------
rm(list=ls())


#################################################################################
# LOAD DATASETS
#################################################################################
#
fcirTR = read.table("./Assignment_1/Data/SimDataCIRCLETR.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
fcirTR$Y = as.factor(fcirTR$Y)

## Exploratory analysis -------------------------------------------------------------------------------------
ggpairs(fcirTR, aes(colour = Y, alpha = 0.4))
ggplot(fcirTR)+geom_point(aes(x=X1,y=X2,color=Y))

fcirTS = read.table("./Assignment_1/Data/SimDataCIRCLETS.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
fcirTS$Y = as.factor(fcirTS$Y)

## Exploratory analysis -------------------------------------------------------------------------------------
ggpairs(fcirTR, aes(colour = Y, alpha = 0.4))
ggplot(fcirTS)+geom_point(aes(x=X1,y=X2,color=Y))

#################################################################################
# SET TRAINCONTROLS for Caret
#################################################################################

set.seed(1999) 
ratioTR = 1
trainIndex <- createDataPartition(fcirTR$Y,      #output variable. createDataPartition creates proportional partitions
                                  p = ratioTR,  #split probability
                                  list = FALSE, #Avoid output as a list
                                  times = 1)    #only one partition

# Sets de TR
fTR = fcirTR[trainIndex,]
# with cross-validation
ctrl_tune <- trainControl(method = "cv",                        #k-fold cross-validation
                          number = 10,                          #Number of folds
                          summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples
                          classProbs = TRUE,                    #Compute class probs in Hold-out samples
                          returnResamp = "all",                 #Return all information about resampling
                          savePredictions = TRUE)               #Compute class probs in Hold-out samples

#################################################################################
# FIT BASIC CLASSIFICATION TREE (THE NAIVE APPROACH?)
#################################################################################
tree.fit = train(fTR[,c("X1","X2","X3","X4","X5","X6")], #Input variables. Other option: fdata[,1:2]
                         y = fTR$Y, 
                         method = "rpart",    
                         parms = list(split = "gini"),          # impuriry measure
                         #tuneGrid = data.frame(cp = seq(0,0.4,0.01)), # first trial
                         tuneGrid = data.frame(cp = seq(0.0,0.01,0.001)), # detail
                         trControl = ctrl_tune,    #ctrl_tune
                         metric = "Accuracy")
tree.fit #information about the resampling settings
summary(tree.fit)  #information about the model trained
tree.fit$finalModel #Cuts performed and nodes. Also shows the number and percentage of cases in each node.

#Plot the tree:
plot(tree.fit$finalModel, uniform=TRUE,margin=0.2)
text(tree.fit$finalModel, use.n=TRUE, all=TRUE, cex=0.8)

#Measure for variable importance
varImp(tree.fit,scale = FALSE)
plot(varImp(tree.fit,scale = FALSE))
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and validation sets
#training
fTR_eval = fTR
fTR_eval$tree_prob = predict(tree.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$tree_pred = predict(tree.fit, type="raw" , newdata = fTR) # predict classes 
#test
fTS <- fcirTS
fTS_eval = fTS
fTS_eval$tree_prob = predict(tree.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$tree_pred = predict(tree.fit, type="raw" , newdata = fTS) # predict classes 

## Performance measures: confusion matices
# Training
confusionMatrix(data = fTR_eval$tree_pred, reference = fTR_eval$Y, positive = "I")
# Test
confusionMatrix(fTS_eval$tree_pred,  fTS_eval$Y, positive = "I")
#################################################################################
# FIT BAGGED TREE 
#################################################################################

set.seed(150) #For replication
#Train model using boostrapped trained data
BAGtree.fit = train(fTR[,c("X1","X2","X3", "X4", "X5", "X6")], #Input variables. Other option: fdata[,1:2]
                    y = fTR$Y, 
                    method = "treebag",   
                    trControl = ctrl_tune, 
                    metric = "Accuracy")

BAGtree.fit
summary(BAGtree.fit)

#Measure for variable importance
varImp(BAGtree.fit,scale = FALSE)
plot(varImp(BAGtree.fit,scale = FALSE))

## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and validation sets
#training
#fTR_eval = fTR
fTR_eval$BAGtree_prob = predict(BAGtree.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$BAGtree_pred = predict(BAGtree.fit, type="raw" , newdata = fTR) # predict classes 
#validation
#fTV_eval = fTV
fTS_eval$BAGtree_prob = predict(BAGtree.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$BAGtree_pred = predict(BAGtree.fit, type="raw" , newdata = fTS) # predict classes 

#######confusion matices
# Training
confusionMatrix(data = fTR_eval$BAGtree_pred, reference = fTR_eval$Y, positive = "I")
# Test
confusionMatrix(fTS_eval$BAGtree_pred, fTS_eval$Y, positive = "I")

# 
# #######Classification performance plots 
# # Training
PlotClassPerformance(fTR_eval$Y,       #Real observations
                     fTR_eval$BAGtree_prob,  #predicted probabilities
                     selClass = "I") #Class to be analyzed
# # Validation
PlotClassPerformance(fTS_eval$Y,       #Real observations
                     fTS_eval$BAGtree_prob,  #predicted probabilities
                     selClass = "I") #Class to be analyzed)

#################################################################################
# FIT RANDOM FORESTS 
#################################################################################
set.seed(150) #For replication
#Training the model
RFtree.fit = train(fTR[,c("X1","X2","X3","X4", "X5", "X6")],
                   y = fTR$Y, # output variable
                   method = "rf",
                   ntree = 100,  
                   tuneGrid = data.frame(mtry = seq(1,ncol(fTR)-1)), # m parameter          
                   trControl = ctrl_tune, #Resampling settings 
                   metric = "Accuracy")    #Summary metrics
# See the forest
RFtree.fit

varImp(RFtree.fit,scale = FALSE)
plot(varImp(RFtree.fit,scale = FALSE))

#Evaluate the model with training and validation sets
#training
#fTR_eval = fTR
fTR_eval$RFtree_prob = predict(RFtree.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$RFtree_pred = predict(RFtree.fit, type="raw" , newdata = fTR) # predict classes 
#validation
#fTV_eval = fTV
fTS_eval$RFtree_prob = predict(RFtree.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$RFtree_pred = predict(RFtree.fit, type="raw" , newdata = fTS) # predict classes 

# Training
confusionMatrix(data = fTR_eval$RFtree_pred, reference = fTR_eval$Y, positive = "I")

# Test
confusionMatrix(fTS_eval$RFtree_pred, fTS_eval$Y,  positive = "I")

# #######Classification performance plots 
# # Training
PlotClassPerformance(fTR_eval$Y, fTR_eval$RFtree_prob,  selClass = "I")
# # Test
PlotClassPerformance(fTS_eval$Y, fTS_eval$RFtree_prob,  selClass = "I")
#################################################################################
# FIT GRADIENT BOOSTING 
#################################################################################
seedTRTVpartition = 150

gbmGrid <- expand.grid(.n.trees = seq(100, 1000, by = 50),
                       .interaction.depth = seq(1, 10, by = 1), 
                       .shrinkage = c(0.01, 0.1), # two typical values
                       .n.minobsinnode = c(1))

# Train the GBM
set.seed(150)
GBM.fit =  train(fTR[,c("X1","X2","X3","X4","X5","X6")], #Input variables. Other option: fdata[,1:2]
                 y = fTR$Y,
                 method = "gbm",
                 tuneGrid = gbmGrid,
                 trControl = ctrl_tune,    #use cv
                 metric = "Accuracy",
                 verbose = FALSE) # The gbm() function produces copious amounts of output, avoid printing a lot

GBM.fit #information about the resampling settings
plot(GBM.fit)

#Measure for variable importance
varImp(GBM.fit,scale = FALSE)
plot(varImp(GBM.fit,scale = FALSE))

#Plot OOB
plot(GBM.fit$finalModel$oobag.improve) # shows the oobag improvement evolving with n.trees

GBM.fit;

## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and validation sets
#training
fTR_eval$GBtree_prob = predict(GBM.fit, type="prob" , newdata = fTR) # predict probabilities
fTR_eval$GBtree_pred = predict(GBM.fit, type="raw" , newdata = fTR) # predict classes 
#validation
fTS_eval$GBtree_prob = predict(GBM.fit, type="prob" , newdata = fTS) # predict probabilities
fTS_eval$GBtree_pred = predict(GBM.fit, type="raw" , newdata = fTS) # predict classes 

## Performance measures: confusion matices
confusionMatrix(fTR_eval$GBtree_pred, fTR_eval$Y, positive = "I") # Training
confusionMatrix(fTS_eval$GBtree_pred,  fTS_eval$Y, positive = "I")# Validation

#################################################################################
# COMPARE USING TS 
#################################################################################

library(caret)
library(ggplot2)
library(ROCR)
library(MLTools)
library(GGally)
library(dplyr)
library(tidyverse)


#TEST
#Overall accuracy
confusionMatrix(fTS_eval$tree_pred, fTS_eval$Y, positive = "I")$overall[1]
confusionMatrix(fTS_eval$BAGtree_pred, fTS_eval$Y, positive = "I")$overall[1]
confusionMatrix(fTS_eval$RFtree_pred, fTS_eval$Y, positive = "I")$overall[1]
confusionMatrix(fTS_eval$GBtree_pred, fTS_eval$Y, positive = "I")$overall[1]


#Calibration curve     
calPlotData <- calibration(Y ~ tree_prob$I + BAGtree_prob$I + RFtree_prob$I + GBtree_prob$I, class = "I",cuts = 6, data = fTS_eval)
xyplot(calPlotData, auto.key = list(columns = 2))

#ROC curve
library(pROC)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$tree_prob$I)
plot(reducedRoc, col="blue")
auc(reducedRoc)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$BAGtree_prob$I)
plot(reducedRoc, add=TRUE, col="red")
auc(reducedRoc)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$RFtree_prob$I)
plot(reducedRoc, add=TRUE, col="orange")
auc(reducedRoc)
reducedRoc <- roc(response = fTS_eval$Y, fTS_eval$GBtree_prob$I)
plot(reducedRoc, add=TRUE, col="green")
auc(reducedRoc)
legend("bottomright", legend=c("Original tree", "Bagged Tree", "Random Forest","Gradient Boosting"), col=c("blue", "red", "orange","green"), lwd=2)
